#!/usr/bin/env python3
"""
Test script for the AI Excel Interviewer System
This script tests the core functionality without the Streamlit UI
"""

import asyncio
import sys
import os
from datetime import datetime

# Add the current directory to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.interview.integrated_interview_engine import IntegratedInterviewEngine
from src.utils.config import settings
from src.utils.logger import setup_logging, get_logger

# Setup logging
setup_logging()
logger = get_logger(__name__)

async def test_basic_interview():
    """Test basic interview functionality"""
    print("ğŸš€ Starting AI Excel Interviewer System Test...")
    print("=" * 60)
    
    try:
        # Initialize the interview engine
        print("ğŸ”„ Initializing interview engine...")
        engine = IntegratedInterviewEngine()
        print("âœ… Interview engine initialized successfully")
        
        # Start interview
        print("\nğŸ¯ Starting interview session...")
        result = await engine.start_interview("Test Candidate", "intermediate")
        
        print(f"âœ… Interview started successfully")
        print(f"ğŸ“‹ Session ID: {result['session_id']}")
        print(f"ğŸ¤– Introduction: {result['introduction'][:100]}...")
        print(f"â“ First Question: {result['first_question']}")
        
        # Test a few interactions
        test_responses = [
            "I have about 3 years of experience with Excel. I use it regularly for data analysis, creating reports, and building dashboards. I'm comfortable with formulas like VLOOKUP, pivot tables, and basic charts.",
            "I recently worked on a sales analysis project where I had to combine data from multiple sources, create pivot tables to summarize the information, and build interactive dashboards for management review.",
            "I would first clean the data by removing duplicates and handling missing values. Then I'd use pivot tables to analyze trends, create charts to visualize key metrics, and build a summary dashboard with key performance indicators."
        ]
        
        print(f"\nğŸ§ª Testing {len(test_responses)} sample interactions...")
        
        for i, response in enumerate(test_responses, 1):
            print(f"\n--- Interaction {i} ---")
            print(f"ğŸ‘¤ User: {response[:50]}...")
            
            # Process interaction
            interaction_result = await engine.process_interaction(response, {})
            
            print(f"ğŸ¤– AI Response: {interaction_result['response'][:100]}...")
            print(f"ğŸ“Š State: {interaction_result['state']}")
            
            if 'evaluation' in interaction_result:
                eval_obj = interaction_result['evaluation']
                print(f"ğŸ“ˆ Score: {eval_obj.total_score:.1f}% | Level: {eval_obj.skill_level}")
            
            if 'challenge' in interaction_result:
                challenge = interaction_result['challenge']
                print(f"ğŸ® Challenge Generated: {challenge.challenge_type.value} (Difficulty: {challenge.difficulty.value})")
        
        # Test challenge completion
        print(f"\nğŸ® Testing challenge completion...")
        challenge_response = "I would use a combination of INDEX and MATCH functions to create a dynamic lookup that can handle the data analysis requirements."
        
        challenge_result = await engine.process_interaction(challenge_response, {
            "challenge_active": True,
            "current_challenge": interaction_result.get("challenge")
        })
        
        print(f"âœ… Challenge completed")
        print(f"ğŸ“ Result: {challenge_result['response'][:100]}...")
        
        # Generate final feedback
        print(f"\nğŸ“Š Generating comprehensive feedback...")
        feedback_result = await engine.process_interaction("I'm ready for feedback", {})
        
        print(f"âœ… Feedback generated")
        print(f"ğŸ’¬ Feedback: {feedback_result['response'][:150]}...")
        
        # Get session summary
        session_data = engine.get_session_data()
        summary = engine._generate_session_summary()
        
        print(f"\nğŸ“‹ Session Summary:")
        print(f"   â€¢ Duration: {summary.get('duration_minutes', 0):.1f} minutes")
        print(f"   â€¢ Questions Asked: {summary.get('total_questions', 0)}")
        print(f"   â€¢ Challenges Completed: {summary.get('total_challenges', 0)}")
        print(f"   â€¢ Average Score: {summary.get('average_score', 0):.1f}%")
        print(f"   â€¢ Skill Level: {summary.get('skill_level', 'Unknown')}")
        
        print(f"\nâœ… All tests completed successfully!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed with error: {str(e)}")
        logger.error(f"System test failed: {str(e)}", exc_info=True)
        return False

async def test_evaluation_system():
    """Test the evaluation system independently"""
    print("\nğŸ” Testing Evaluation System...")
    
    try:
        from src.evaluation.answer_evaluator import AnswerEvaluator
        
        evaluator = AnswerEvaluator()
        
        test_cases = [
            {
                "question": "How would you use VLOOKUP in Excel?",
                "answer": "VLOOKUP is used to search for a value in the first column of a range and return a value in the same row from another column. For example, =VLOOKUP(A2, Sheet2!A:B, 2, FALSE) would look up the value in A2 in the first column of Sheet2's A:B range and return the corresponding value from the second column.",
                "expected_score_range": (80, 100)
            },
            {
                "question": "What's the difference between COUNT and COUNTA?",
                "answer": "COUNT counts only numeric values, while COUNTA counts all non-empty cells including text and numbers.",
                "expected_score_range": (70, 90)
            },
            {
                "question": "How do you create a pivot table?",
                "answer": "I don't know, I've never used pivot tables.",
                "expected_score_range": (0, 30)
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n--- Test Case {i} ---")
            print(f"â“ Question: {test_case['question']}")
            print(f"ğŸ‘¤ Answer: {test_case['answer'][:50]}...")
            
            evaluation = await evaluator.evaluate_answer(
                test_case["question"],
                test_case["answer"],
                "technical"
            )
            
            print(f"ğŸ“Š Score: {evaluation.total_score:.1f}%")
            print(f"ğŸ¯ Skill Level: {evaluation.skill_level}")
            
            # Check if score is in expected range
            min_score, max_score = test_case["expected_score_range"]
            if min_score <= evaluation.total_score <= max_score:
                print(f"âœ… Score within expected range ({min_score}-{max_score})")
            else:
                print(f"âš ï¸ Score outside expected range ({min_score}-{max_score})")
            
            # Show dimension breakdown
            print(f"ğŸ“ˆ Dimension Scores:")
            for dim, score in evaluation.dimension_scores.items():
                print(f"   â€¢ {dim.value.replace('_', ' ').title()}: {score.score}/25")
        
        print(f"\nâœ… Evaluation system test completed!")
        return True
        
    except Exception as e:
        print(f"âŒ Evaluation test failed: {str(e)}")
        logger.error(f"Evaluation test failed: {str(e)}", exc_info=True)
        return False

async def test_excel_simulator():
    """Test the Excel simulator"""
    print("\nğŸ”¢ Testing Excel Simulator...")
    
    try:
        from src.simulation.excel_simulator import ExcelSimulator, ChallengeType, DifficultyLevel
        
        simulator = ExcelSimulator()
        
        # Test challenge generation
        print("ğŸ¯ Testing challenge generation...")
        challenge = simulator.generate_challenge(ChallengeType.DATA_ANALYSIS, DifficultyLevel.INTERMEDIATE)
        
        print(f"âœ… Challenge generated:")
        print(f"   â€¢ Type: {challenge.challenge_type.value}")
        print(f"   â€¢ Difficulty: {challenge.difficulty.value}")
        print(f"   â€¢ Description: {challenge.description[:50]}...")
        print(f"   â€¢ Expected Time: {challenge.expected_time_minutes} minutes")
        
        # Test dataset generation
        print("\nğŸ“Š Testing dataset generation...")
        dataset = simulator.generate_dataset(challenge.dataset_config)
        
        print(f"âœ… Dataset generated:")
        print(f"   â€¢ Size: {len(dataset.data)} rows")
        print(f"   â€¢ Columns: {len(dataset.data[0]) if dataset.data else 0}")
        print(f"   â€¢ Data Preview: {str(dataset.data[0])[:50]}..." if dataset.data else "No data")
        
        # Test solution evaluation
        print("\nğŸ” Testing solution evaluation...")
        test_solution = {
            "solution_text": "I would use pivot tables to analyze the data and create charts to visualize trends.",
            "time_taken": 180  # 3 minutes
        }
        
        result = simulator.evaluate_challenge_solution(challenge, test_solution)
        
        print(f"âœ… Solution evaluated:")
        print(f"   â€¢ Total Score: {result.total_score:.1f}%")
        print(f"   â€¢ Correctness: {result.correctness_score:.1f}%")
        print(f"   â€¢ Efficiency: {result.efficiency_score:.1f}%")
        print(f"   â€¢ Approach: {result.approach_score:.1f}%")
        print(f"   â€¢ Feedback: {result.feedback}")
        
        print(f"\nâœ… Excel simulator test completed!")
        return True
        
    except Exception as e:
        print(f"âŒ Excel simulator test failed: {str(e)}")
        logger.error(f"Excel simulator test failed: {str(e)}", exc_info=True)
        return False

async def main():
    """Main test function"""
    print("ğŸš€ AI Excel Interviewer System - Comprehensive Test Suite")
    print("=" * 70)
    print("This test will verify all core components of the system:")
    print("â€¢ Interview Engine Integration")
    print("â€¢ Answer Evaluation System")
    print("â€¢ Excel Simulator")
    print("â€¢ Conversation Flow")
    print("â€¢ Challenge Generation")
    print("â€¢ Feedback Generation")
    print("=" * 70)
    
    # Run all tests
    test_results = []
    
    # Test 1: Evaluation System
    print("\n" + "="*50)
    test_results.append(await test_evaluation_system())
    
    # Test 2: Excel Simulator
    print("\n" + "="*50)
    test_results.append(await test_excel_simulator())
    
    # Test 3: Full Integration
    print("\n" + "="*50)
    test_results.append(await test_basic_interview())
    
    # Summary
    print("\n" + "="*70)
    print("ğŸ“Š TEST RESULTS SUMMARY")
    print("="*70)
    
    test_names = ["Evaluation System", "Excel Simulator", "Full Integration"]
    
    for i, (name, result) in enumerate(zip(test_names, test_results)):
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"{i+1}. {name:<20} {status}")
    
    passed = sum(test_results)
    total = len(test_results)
    
    print(f"\nğŸ“ˆ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ All tests passed! The system is ready for deployment.")
        print("\nğŸš€ To run the Streamlit prototype:")
        print("   streamlit run prototype_demo.py")
    else:
        print("âš ï¸ Some tests failed. Please review the logs and fix issues before deployment.")
    
    return passed == total

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)